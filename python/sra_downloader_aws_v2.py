#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SRA Pro Downloader V2.1
åŠŸèƒ½ï¼šNCBI SRA æ•°æ®é«˜é€Ÿä¸‹è½½å·¥å…· (åŸºäº AWS S3)
ç‰¹æ€§ï¼šæ–­ç‚¹ç»­ä¼ ã€MD5æ ¡éªŒã€æ™ºèƒ½é‡è¯•ã€è¯¦ç»†æ—¥å¿—ã€æ–‡ä»¶çº§å¹¶å‘
"""

import os
import sys
import re
import json
import time
import threading
import hashlib
import argparse
import requests
import boto3
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from botocore import UNSIGNED
from botocore.config import Config
from tqdm import tqdm
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from rich.console import Console
from rich_argparse import RichHelpFormatter

# ============================
# 1. é…ç½®ä¸å·¥å…·å‡½æ•°
# ============================

def setup_logging(log_file="sra_download.log"):
    """é…ç½®æ—¥å¿—ï¼šå±å¹•æ˜¾ç¤ºINFOï¼Œæ–‡ä»¶è®°å½•DEBUG"""
    logger.remove()
    # æ§åˆ¶å°è¾“å‡ºï¼šç®€æ´æ ¼å¼
    logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")
    # æ–‡ä»¶è¾“å‡ºï¼šè¯¦ç»†æ ¼å¼ï¼Œæ”¯æŒè½®è½¬
    logger.add(log_file, rotation="10 MB", retention="7 days", level="DEBUG", 
               format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {process.name} | {message}")

def calculate_md5(file_path):
    """æµå¼è®¡ç®—å¤§æ–‡ä»¶ MD5ï¼Œé¿å…å†…å­˜æº¢å‡º"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            # æ¯æ¬¡è¯»å– 8MB
            for chunk in iter(lambda: f.read(8 * 1024 * 1024), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except FileNotFoundError:
        return None

def format_speed(size_bytes, duration_seconds):
    """è®¡ç®—å¹¶æ ¼å¼åŒ–é€Ÿåº¦"""
    if duration_seconds <= 0: return "N/A"
    speed = (size_bytes / 1024 / 1024) / duration_seconds
    return f"{speed:.2f} MB/s"

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´"""
    return f"{seconds:.2f} s"

# ============================
# 2. æ ¸å¿ƒç±»å®šä¹‰
# ============================

class SraMetadata:
    def __init__(self, s3_uri, md5, size):
        self.s3_uri = s3_uri
        self.md5 = md5
        self.size = int(size)

class SraUtils:
    @staticmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def get_metadata(run_id, api_key=None):
        """è·å– S3 åœ°å€å’Œ MD5 å€¼ (å¸¦é‡è¯•æœºåˆ¶)"""
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {"db": "sra", "id": run_id, "rettype": "full", "retmode": "xml"}
        if api_key:
            params["api_key"] = api_key
        
        logger.info(f"[{run_id}] æ­£åœ¨è·å–å…ƒæ•°æ®...")
        resp = requests.get(url, params=params, timeout=30)
        resp.raise_for_status()
        
        root = ET.fromstring(resp.text)
        
        # 1. å¯»æ‰¾ S3 é“¾æ¥
        target_url = None
        for alt in root.findall(".//Alternatives"):
            if alt.get('org') == 'AWS' and alt.get('free_egress') == 'worldwide':
                target_url = alt.get('url')
                break
        
        if not target_url:
            logger.warning(f"[{run_id}] æœªæ‰¾åˆ°æ˜ç¡®çš„ AWS worldwide é“¾æ¥")
            return None

        # è½¬æ¢ s3 é“¾æ¥
        match = re.match(r'https://([^.]+)\.s3\.amazonaws\.com/(.+)', target_url)
        if match:
            s3_uri = f"s3://{match.group(1)}/{match.group(2)}"
        else:
            s3_uri = target_url.replace("https://", "s3://").replace(".s3.amazonaws.com", "")

        # 2. å¯»æ‰¾ MD5 å’Œ æ–‡ä»¶å¤§å°
        expected_md5 = None
        file_size = 0
        
        filename = s3_uri.split('/')[-1]
        for sra_file in root.findall(".//SRAFile"):
            if sra_file.get('filename') == filename:
                expected_md5 = sra_file.get('md5')
                file_size = sra_file.get('size')
                break
        
        if not file_size:
             # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å– Run èŠ‚ç‚¹ä¿¡æ¯
             run_node = root.find(".//RUN")
             if run_node is not None:
                 file_size = run_node.get('size', 0)

        logger.debug(f"[{run_id}] å…ƒæ•°æ®è§£ææˆåŠŸ: Size={file_size}, MD5={expected_md5}")
        return SraMetadata(s3_uri, expected_md5, file_size)

class ResumableDownloader:
    def __init__(self, run_id, metadata, save_dir=".", chunk_size_mb=20, max_workers=8):
        self.run_id = run_id
        self.metadata = metadata
        self.bucket, self.key = self._parse_s3_uri(metadata.s3_uri)
        self.filename = os.path.basename(self.key)
        self.save_dir = save_dir
        self.filepath = os.path.join(save_dir, self.filename)
        self.meta_file = self.filepath + ".meta.json"
        
        self.chunk_size = chunk_size_mb * 1024 * 1024
        self.max_workers = max_workers
        self.s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED, max_pool_connections=max_workers))
        self.file_lock = threading.Lock()

        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)

    def _parse_s3_uri(self, uri):
        match = re.match(r's3://([^/]+)/(.+)', uri)
        if match:
            return match.group(1), match.group(2)
        raise ValueError(f"Invalid S3 URI: {uri}")

    def _load_progress(self):
        if os.path.exists(self.meta_file):
            try:
                with open(self.meta_file, 'r') as f:
                    return json.load(f)
            except: 
                pass
        return {"downloaded_chunks": []}

    def _save_progress(self, downloaded_chunks):
        with open(self.meta_file, 'w') as f:
            json.dump({"downloaded_chunks": list(downloaded_chunks)}, f)

    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=1, max=10), 
           retry=retry_if_exception_type((requests.ConnectionError, boto3.exceptions.Boto3Error)))
    def _download_chunk_task(self, start, end, chunk_id, pbar):
        """ä¸‹è½½åˆ†ç‰‡ï¼ˆå¸¦é‡è¯•ï¼‰"""
        range_header = f"bytes={start}-{end}"
        resp = self.s3_client.get_object(Bucket=self.bucket, Key=self.key, Range=range_header)
        content = resp['Body'].read()
        
        with self.file_lock:
            with open(self.filepath, 'r+b') as f:
                f.seek(start)
                f.write(content)
        
        pbar.update(len(content))
        return chunk_id

    def start(self):
        start_time = time.time()
        logger.info(f"[{self.run_id}] å‡†å¤‡ä¸‹è½½: {self.filename} ({self.metadata.size / (1024**3):.2f} GB)")
        
        # 1. é¢„åˆ†é…æ–‡ä»¶
        if not os.path.exists(self.filepath):
            with open(self.filepath, 'wb') as f:
                f.truncate(self.metadata.size)
        
        # 2. è®¡ç®—åˆ†ç‰‡ä»»åŠ¡
        total_size = self.metadata.size
        num_chunks = (total_size + self.chunk_size - 1) // self.chunk_size
        progress_data = self._load_progress()
        downloaded_chunks = set(progress_data['downloaded_chunks'])
        
        tasks = []
        for i in range(num_chunks):
            if i in downloaded_chunks: continue
            start = i * self.chunk_size
            end = min(start + self.chunk_size - 1, total_size - 1)
            tasks.append((i, start, end))
            
        initial_downloaded = sum([self.chunk_size for _ in downloaded_chunks])
        if initial_downloaded > total_size: initial_downloaded = total_size

        if not tasks:
            logger.info(f"[{self.run_id}] æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼Œç›´æ¥æ ¡éªŒ...")
            return self._verify_integrity(start_time, skipped_download=True)

        logger.info(f"[{self.run_id}] å‰©ä½™åˆ†ç‰‡: {len(tasks)}/{num_chunks} | çº¿ç¨‹: {self.max_workers}")

        # è¿›åº¦æ¡
        pbar = tqdm(total=total_size, initial=initial_downloaded, unit='B', unit_scale=True, 
                   desc=f"{self.run_id}", ncols=100, position=0, leave=True)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_chunk = {
                executor.submit(self._download_chunk_task, s, e, i, pbar): i 
                for i, s, e in tasks
            }
            
            completed_count = 0
            for future in as_completed(future_to_chunk):
                try:
                    chunk_id = future.result()
                    downloaded_chunks.add(chunk_id)
                    completed_count += 1
                    # æ¯10ä¸ªåˆ†ç‰‡ä¿å­˜ä¸€æ¬¡è¿›åº¦ï¼Œå‡å°‘IO
                    if completed_count % 10 == 0:
                        self._save_progress(downloaded_chunks)
                except Exception as e:
                    tqdm.write(f"âŒ [{self.run_id}] åˆ†ç‰‡ä¸‹è½½å¤±è´¥: {e}")
        
        pbar.close()
        self._save_progress(downloaded_chunks)

        if len(downloaded_chunks) == num_chunks:
            return self._verify_integrity(start_time, skipped_download=False)
        else:
            logger.error(f"[{self.run_id}] ä¸‹è½½æœªå®Œæˆï¼Œéƒ¨åˆ†åˆ†ç‰‡å¤±è´¥ã€‚")
            return False

    def _verify_integrity(self, start_time, skipped_download=False):
        """æ ¡éªŒ MD5 å¹¶è®°å½•æœ€ç»ˆæ—¥å¿—"""
        download_end_time = time.time()
        download_duration = download_end_time - start_time
        
        if not self.metadata.md5:
            logger.warning(f"[{self.run_id}] XMLæœªæä¾›MD5ï¼Œè·³è¿‡æ ¡éªŒã€‚")
            logger.success(f"[{self.run_id}] ä»»åŠ¡å®Œæˆ (æ— æ ¡éªŒ). è€—æ—¶: {format_time(download_duration)}")
            if os.path.exists(self.meta_file): os.remove(self.meta_file)
            return True

        logger.info(f"[{self.run_id}] æ­£åœ¨æ ¡éªŒ MD5 (æœ¬åœ°è®¡ç®—ä¸­)...")
        verify_start = time.time()
        local_md5 = calculate_md5(self.filepath)
        verify_duration = time.time() - verify_start
        
        if local_md5 == self.metadata.md5:
            speed_info = ""
            if not skipped_download:
                speed_str = format_speed(self.metadata.size, download_duration)
                speed_info = f" | é€Ÿåº¦: {speed_str}"
            
            logger.success(f"[{self.run_id}] âœ… æ ¡éªŒé€šè¿‡! æ€»è€—æ—¶: {format_time(download_duration + verify_duration)} (ä¸‹è½½: {format_time(download_duration)}{speed_info}, æ ¡éªŒ: {format_time(verify_duration)})")
            
            # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ä¾›ç»Ÿè®¡
            logger.debug(f"STATS | ID={self.run_id} | Size={self.metadata.size} | Time={download_duration:.2f}s | Speed={speed_info}")
            
            if os.path.exists(self.meta_file): os.remove(self.meta_file)
            return True
        else:
            logger.critical(f"[{self.run_id}] âŒ æ ¡éªŒå¤±è´¥! æœ¬åœ°:{local_md5} != è¿œç¨‹:{self.metadata.md5}")
            return False

# ============================
# 3. ä»»åŠ¡è°ƒåº¦é€»è¾‘
# ============================

def process_single_run(run_id, args):
    """å•ä¸ªä¸‹è½½ä»»åŠ¡çš„å…¥å£å‡½æ•°ï¼ˆä¾›å¤šè¿›ç¨‹è°ƒç”¨ï¼‰"""
    try:
        # 1. è·å–å…ƒæ•°æ®
        metadata = SraUtils.get_metadata(run_id, api_key=args.api_key)
        if not metadata:
            logger.error(f"[{run_id}] æ— æ³•è·å–æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥")
            return False
        
        # 2. å¯åŠ¨ä¸‹è½½
        downloader = ResumableDownloader(
            run_id=run_id,
            metadata=metadata,
            save_dir=args.outdir,
            max_workers=args.threads,     # æ¯ä¸ªæ–‡ä»¶å†…éƒ¨çš„åˆ†ç‰‡ä¸‹è½½çº¿ç¨‹æ•°
            chunk_size_mb=args.chunk_size
        )
        return downloader.start()
        
    except Exception as e:
        logger.exception(f"[{run_id}] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯")
        return False

# ============================
# 4. CLI ä¸»å…¥å£
# ============================

def main():
    # å±€éƒ¨å¯¼å…¥ï¼Œç¡®ä¿åªæœ‰åœ¨è¿è¡Œæ—¶æ‰éœ€è¦è¿™äº›åº“
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.text import Text
        from rich_argparse import RichHelpFormatter
    except ImportError:
        print("ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œè¯·å®‰è£…ç¾åŒ–åº“: pip install rich rich-argparse")
        sys.exit(1)

    # --- 1. å®šä¹‰ Help ç•Œé¢æ ·å¼ ---
    # è®©å‚æ•°åæ˜¾ç¤ºä¸ºé’è‰²ï¼Œåˆ†ç»„æ ‡é¢˜æ˜¾ç¤ºä¸ºåŠ ç²—é»„è‰²
    RichHelpFormatter.styles["argparse.args"] = "cyan"
    RichHelpFormatter.styles["argparse.groups"] = "bold yellow"
    RichHelpFormatter.styles["argparse.help"] = "white"
    RichHelpFormatter.styles["argparse.metavar"] = "bold magenta"

    parser = argparse.ArgumentParser(
        description="[bold green]NCBI SRA Pro Downloader V2.1[/]\n"
                    "åŸºäº AWS S3 çš„é«˜é€Ÿç”Ÿç‰©æ•°æ®ä¸‹è½½å™¨ï¼Œæ”¯æŒ [bold red]æ–­ç‚¹ç»­ä¼ [/] & [bold red]MD5æ ¡éªŒ[/]",
        formatter_class=RichHelpFormatter,
        epilog="[dim]Example: python sra_download.py SRR32730731 -o ./data -p 4[/]"
    )

    # --- 2. å‚æ•°åˆ†ç»„ (è®© -h ç•Œé¢æ›´æ¸…æ™°) ---
    
    # [æ ¸å¿ƒå‚æ•°]
    req_group = parser.add_argument_group("ğŸ”¥ æ ¸å¿ƒå‚æ•°")
    req_group.add_argument("ids", nargs="+", help="SRA Run IDs (æ”¯æŒå¤šä¸ªï¼Œä¾‹å¦‚: [green]SRR32730731 SRR32730732[/])")

    # [å¸¸ç”¨é€‰é¡¹]
    opt_group = parser.add_argument_group("ğŸ“‚ å¸¸ç”¨é€‰é¡¹")
    opt_group.add_argument("-o", "--outdir", default=".", 
                           help="ä¸‹è½½ä¿å­˜ç›®å½• (é»˜è®¤: [u]./[/])")
    
    # [æ€§èƒ½è°ƒä¼˜]
    perf_group = parser.add_argument_group("ğŸš€ æ€§èƒ½è°ƒä¼˜")
    perf_group.add_argument("-p", "--parallel-files", type=int, default=1, metavar="N",
                           help="[bold]æ–‡ä»¶çº§å¹¶å‘æ•°[/]ï¼šåŒæ—¶ä¸‹è½½çš„æ–‡ä»¶æ•°é‡ (é»˜è®¤: 1, å³ä¸²è¡Œ)")
    perf_group.add_argument("-t", "--threads", type=int, default=8, metavar="N",
                           help="[bold]çº¿ç¨‹çº§å¹¶å‘æ•°[/]ï¼šå•æ–‡ä»¶å†…éƒ¨åˆ†ç‰‡ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 8)")
    perf_group.add_argument("--chunk-size", type=int, default=20, metavar="MB",
                           help="åˆ†ç‰‡å¤§å° (é»˜è®¤: 20 MB)")

    # [å…¶ä»–è®¾ç½®]
    misc_group = parser.add_argument_group("âš™ï¸ å…¶ä»–è®¾ç½®")
    misc_group.add_argument("--api-key", metavar="KEY", 
                           help="NCBI API Key (ç”¨äºæå‡ API é™æµé˜ˆå€¼)")
    misc_group.add_argument("--log", default="sra_download.log", metavar="FILE",
                           help="æ—¥å¿—æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()
    
    # --- 3. åˆå§‹åŒ–ä¸ Banner æ˜¾ç¤º ---
    
    setup_logging(args.log)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)
        
    console = Console()
    
    # æ˜¾ç¤ºå¯åŠ¨é¢æ¿
    summary_text = Text()
    summary_text.append(f"ğŸ“¦ å¾…ä¸‹è½½æ–‡ä»¶: {len(args.ids)} ä¸ª\n", style="bold white")
    summary_text.append(f"ğŸ“‚ ä¿å­˜ç›®å½•: {os.path.abspath(args.outdir)}\n", style="cyan")
    summary_text.append(f"ğŸš€ å¹¶å‘é…ç½®: {args.parallel_files} æ–‡ä»¶ x {args.threads} çº¿ç¨‹", style="green")
    
    console.print(Panel(summary_text, title="[bold green]SRA Downloader ä»»åŠ¡å¯åŠ¨[/]", expand=False))

    start_time_all = time.time()
    total_files = len(args.ids)
    
    # è®°å½•åˆ° Loguru æ–‡ä»¶æ—¥å¿— (ä¿ç•™æŠ€æœ¯ç»†èŠ‚)
    logger.info(f"CLIå¯åŠ¨å‚æ•°: {args}")

    failed_ids = []
    
    # --- 4. ä»»åŠ¡æ‰§è¡Œé€»è¾‘ (ä¸²è¡Œ vs å¹¶è¡Œ) ---
    
    try:
        if args.parallel_files > 1 and total_files > 1:
            # === å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½ ===
            console.print(f"[bold yellow]âš¡ å¯ç”¨å¤šè¿›ç¨‹æ¨¡å¼ (Pool Size: {args.parallel_files})[/]")
            
            with ProcessPoolExecutor(max_workers=args.parallel_files) as executor:
                # æäº¤ä»»åŠ¡
                future_to_id = {executor.submit(process_single_run, run_id, args): run_id for run_id in args.ids}
                
                for future in as_completed(future_to_id):
                    run_id = future_to_id[future]
                    try:
                        success = future.result()
                        if not success:
                            failed_ids.append(run_id)
                    except Exception as e:
                        logger.error(f"[{run_id}] è¿›ç¨‹å¼‚å¸¸: {e}")
                        failed_ids.append(run_id)
        else:
            # === ä¸²è¡Œä¸‹è½½ ===
            if total_files > 1:
                console.print("[dim]æç¤º: ä½¿ç”¨ -p å‚æ•°å¯å¼€å¯å¤šæ–‡ä»¶å¹¶è¡Œä¸‹è½½[/]")
                
            for i, run_id in enumerate(args.ids, 1):
                console.print(f"\n[bold]Processing {i}/{total_files}: {run_id}[/]")
                success = process_single_run(run_id, args)
                if not success:
                    failed_ids.append(run_id)

    except KeyboardInterrupt:
        console.print("\n[bold red]âš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œï¼æ­£åœ¨å®‰å…¨é€€å‡º...[/]")
        sys.exit(130)

    # --- 5. æœ€ç»ˆç»Ÿè®¡ ---
    total_duration = time.time() - start_time_all
    
    console.print("\n")
    if failed_ids:
        console.print(Panel(f"âŒ å¤±è´¥åˆ—è¡¨: {', '.join(failed_ids)}", title="ä»»åŠ¡éƒ¨åˆ†å¤±è´¥", style="bold red"))
    else:
        console.print(Panel(f"âœ… æ‰€æœ‰ {total_files} ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸï¼\nâ±ï¸ æ€»è€—æ—¶: {format_time(total_duration)}", 
                            title="ä»»åŠ¡å®Œæˆ", style="bold green"))

if __name__ == "__main__":
    main()